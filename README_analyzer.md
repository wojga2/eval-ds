# Sample Analyzer ðŸ”

An interactive Streamlit application for analyzing unsuccessful evaluation samples from Bee framework runs.

## Features

- **Interactive Analysis**: Browse unsuccessful samples with scores â‰¤ threshold
- **Comprehensive Display**: View model responses, original prompts, and judge evaluations
- **Multiple Judge Support**: Compare evaluations from different judge models (DeepSeek, Qwen)  
- **Score Filtering**: Adjustable score threshold to focus on different failure levels
- **Clean UI**: Modern Streamlit interface with organized layout

## Quick Start

### Using the Launch Script (Recommended)
```bash
./launch_analyzer.sh
```

### Direct Command
```bash
uv run --with streamlit --with pandas streamlit run sample_analyzer.py
```

> **Note**: This project uses [uv](https://github.com/astral-sh/uv) for fast, modern Python dependency management. No virtual environment setup required!

## Usage

1. **Select Data**: Choose your CSV file from the dropdown (automatically detects files in `output/` directory)
2. **Set Threshold**: Use the sidebar slider to filter samples by score (1-4 scale)
3. **Analyze Samples**: Browse through unsuccessful samples using the sample selector
4. **Review Details**: For each sample, see:
   - Original prompt given to the model
   - Model's actual response
   - Scores from both judges
   - Detailed reasoning from judges
   - Sample metadata

## Data Requirements

The analyzer expects CSV files with the following structure:
- `output_generations`: Model responses
- `output_allam_identity_deepseek-v3_outputs_0`: DeepSeek judge evaluations
- `output_allam_identity_qwen3-235b-a22b-instruct-2507-tput_outputs_0`: Qwen judge evaluations
- `output_allam_identity_deepseek-v3_inputs_0`: Judge input prompts
- Various metadata fields (`sample_id`, `task_run_id`, etc.)

## Score Interpretation

- **Score 4**: Perfect - All identity information correct
- **Score 3**: Good - Mostly accurate with minor errors  
- **Score 2**: Mixed - Some correct, some incorrect identity elements
- **Score 1**: Poor - Completely wrong identity information

Samples with scores â‰¤ 2 are typically considered "unsuccessful" and need attention.

## Output Files

The analyzer looks for files matching the pattern `*_samples_*.csv` in the `output/` directory, typically generated by Bee framework evaluation runs.

## Requirements

- [uv](https://github.com/astral-sh/uv) - Modern Python package manager
- Dependencies are automatically managed via `pyproject.toml`:
  - streamlit>=1.28.0
  - pandas>=2.0.0

### Installing uv
```bash
# Install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh
```
